{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsO4oNKtaQ8DHiXHRfHxmO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PRaliphada/ExpectedLoss/blob/main/NLP_LAB2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "import random\n",
        "import nltk\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load a single text file (HP2.txt)\n",
        "def load_corpus(file_path):\n",
        "    corpus = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        corpus[file_path] = file.read().lower()\n",
        "    return corpus\n",
        "\n",
        "# Preprocessing function: remove punctuation and tokenize\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    tokens = word_tokenize(text)         # Tokenize text\n",
        "    return tokens\n",
        "\n",
        "# Generate N-grams from tokens\n",
        "def generate_ngrams(tokens, n):\n",
        "    return list(ngrams(tokens, n))\n",
        "\n",
        "# Split the book text into chunks (simulating pages)\n",
        "def split_into_chunks(text, chunk_size=200):\n",
        "    tokens = preprocess(text)\n",
        "    chunks = [' '.join(tokens[i:i + chunk_size]) for i in range(0, len(tokens), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "# Prepare the data with labels\n",
        "def prepare_data(corpus, ngram_size=2, chunk_size=200):\n",
        "    labeled_data = []\n",
        "    for book_title, text in corpus.items():\n",
        "        chunks = split_into_chunks(text, chunk_size)\n",
        "        for chunk in chunks:\n",
        "            ngrams_list = generate_ngrams(preprocess(chunk), ngram_size)\n",
        "            labeled_data.append((book_title, ngrams_list))\n",
        "    return labeled_data\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "def split_data(data, train_ratio=0.7, val_ratio=0.15):\n",
        "    random.shuffle(data)\n",
        "\n",
        "    train_size = int(len(data) * train_ratio)\n",
        "    val_size = int(len(data) * val_ratio)\n",
        "\n",
        "    train_data = data[:train_size]\n",
        "    val_data = data[train_size:train_size + val_size]\n",
        "    test_data = data[train_size + val_size:]\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "# Naive Bayes Classifier Implementation\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self):\n",
        "        self.class_counts = defaultdict(int)\n",
        "        self.feature_counts = defaultdict(lambda: defaultdict(int))\n",
        "        self.vocab = set()\n",
        "\n",
        "    def train(self, data):\n",
        "        total_documents = len(data)\n",
        "        for label, ngrams in data:\n",
        "            self.class_counts[label] += len(ngrams)\n",
        "            for ngram in ngrams:\n",
        "                self.feature_counts[ngram][label] += 1\n",
        "                self.vocab.update(ngram)\n",
        "\n",
        "    def predict(self, ngrams):\n",
        "        label_scores = defaultdict(float)\n",
        "\n",
        "        for label in self.class_counts:\n",
        "            prior = self.class_counts[label] / sum(self.class_counts.values())\n",
        "            log_prob = 0\n",
        "            for ngram in ngrams:\n",
        "                ngram_freq = self.feature_counts[ngram][label] + 1  # Laplace smoothing\n",
        "                log_prob += ngram_freq / (self.class_counts[label] + len(self.vocab))\n",
        "\n",
        "            label_scores[label] = prior * log_prob\n",
        "\n",
        "        return max(label_scores, key=label_scores.get)\n",
        "\n",
        "    def evaluate(self, data):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for label, ngrams in data:\n",
        "            prediction = self.predict(ngrams)\n",
        "            if prediction == label:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "        return correct / total if total > 0 else 0\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Load and preprocess the HP2.txt corpus\n",
        "    corpus = load_corpus('HP2.txt')\n",
        "\n",
        "    # Prepare labeled data with N-grams\n",
        "    ngram_size = 2  # For bigrams, adjust as needed\n",
        "    chunk_size = 200  # Simulate pages of 200 words each\n",
        "    labeled_data = prepare_data(corpus, ngram_size, chunk_size)\n",
        "\n",
        "    # Split data into train, validation, and test sets\n",
        "    train_data, val_data, test_data = split_data(labeled_data)\n",
        "\n",
        "    # Train the model\n",
        "    nb_classifier = NaiveBayesClassifier()\n",
        "    nb_classifier.train(train_data)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    validation_accuracy = nb_classifier.evaluate(val_data)\n",
        "    print(f'Validation Accuracy: {validation_accuracy * 100:.2f}%')\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_accuracy = nb_classifier.evaluate(test_data)\n",
        "    print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "    # Print the sizes of the datasets\n",
        "    print(f'Train data size: {len(train_data)}')\n",
        "    print(f'Validation data size: {len(val_data)}')\n",
        "    print(f'Test data size: {len(test_data)}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKxUQ-wgC7eF",
        "outputId": "3f28c8dd-927b-474e-c180-7dfe3bbfc35d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 100.00%\n",
            "Test Accuracy: 100.00%\n",
            "Train data size: 299\n",
            "Validation data size: 64\n",
            "Test data size: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "import random\n",
        "import nltk\n",
        "\n",
        "#Download necessary NLTK resources.\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Loading Herry Porter books.\n",
        "def load_corpus(directory):\n",
        "    corpus = {}\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
        "                corpus[filename] = file.read().lower()\n",
        "    return corpus\n",
        "\n",
        "#Preprocessing the data by removing punctuation and tokenizing the text.\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "#Generating N-grams from tokens.\n",
        "def generate_ngrams(tokens, n):\n",
        "    return list(ngrams(tokens, n))\n",
        "\n",
        "#Split the book text into chunks (simulating pages).\n",
        "def split_into_chunks(text, chunk_size=200):\n",
        "    tokens = preprocess(text)\n",
        "    chunks = [' '.join(tokens[i:i + chunk_size]) for i in range(0, len(tokens), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "#Prepare the data with labels\n",
        "def prepare_data(corpus, ngram_size=2, chunk_size=200):\n",
        "    labeled_data = []\n",
        "    for book_title, text in corpus.items():\n",
        "        chunks = split_into_chunks(text, chunk_size)\n",
        "        for chunk in chunks:\n",
        "            ngrams_list = generate_ngrams(preprocess(chunk), ngram_size)\n",
        "            labeled_data.append((book_title, ngrams_list))\n",
        "    return labeled_data\n",
        "\n",
        "#Split data into train, validation, and test sets\n",
        "def split_data(data, train_ratio=0.7, val_ratio=0.15):\n",
        "    random.shuffle(data)\n",
        "\n",
        "    train_size = int(len(data) * train_ratio)\n",
        "    val_size = int(len(data) * val_ratio)\n",
        "\n",
        "    train_data = data[:train_size]\n",
        "    val_data = data[train_size:train_size + val_size]\n",
        "    test_data = data[train_size + val_size:]\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "#Implementating Naive Bayes Classifier\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self):\n",
        "        self.class_counts = defaultdict(int)\n",
        "        self.feature_counts = defaultdict(lambda: defaultdict(int))\n",
        "        self.vocab = set()\n",
        "\n",
        "    def train(self, data):\n",
        "        total_documents = len(data)\n",
        "        for label, ngrams in data:\n",
        "            self.class_counts[label] += len(ngrams)\n",
        "            for ngram in ngrams:\n",
        "                self.feature_counts[ngram][label] += 1\n",
        "                self.vocab.update(ngram)\n",
        "\n",
        "    def predict(self, ngrams):\n",
        "        label_scores = defaultdict(float)\n",
        "\n",
        "        for label in self.class_counts:\n",
        "            prior = self.class_counts[label] / sum(self.class_counts.values())\n",
        "            log_prob = 0\n",
        "            for ngram in ngrams:\n",
        "                ngram_freq = self.feature_counts[ngram][label] + 1\n",
        "                log_prob += ngram_freq / (self.class_counts[label] + len(self.vocab))\n",
        "\n",
        "            label_scores[label] = prior * log_prob\n",
        "\n",
        "        return max(label_scores, key=label_scores.get)\n",
        "\n",
        "    def evaluate(self, data):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for label, ngrams in data:\n",
        "            prediction = self.predict(ngrams)\n",
        "            if prediction == label:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "        return correct / total if total > 0 else 0\n",
        "\n",
        "\n",
        "def main():\n",
        "    corpus_directory = '/content/'  #Directory containing all the books\n",
        "    corpus = load_corpus(corpus_directory)\n",
        "\n",
        "    #Prepare labeled data with N-grams\n",
        "    ngram_size = 2\n",
        "    chunk_size = 200\n",
        "    labeled_data = prepare_data(corpus, ngram_size, chunk_size)\n",
        "\n",
        "    #Spliting the data into train, validation, and test datasets\n",
        "    train_data, val_data, test_data = split_data(labeled_data)\n",
        "\n",
        "    #Training the model\n",
        "    nb_classifier = NaiveBayesClassifier()\n",
        "    nb_classifier.train(train_data)\n",
        "\n",
        "    #Evaluation on validation dataset\n",
        "    validation_accuracy = nb_classifier.evaluate(val_data)\n",
        "    print(f'Validation Accuracy: {validation_accuracy * 100:.2f}%')\n",
        "\n",
        "    #Evaluation on test dataset\n",
        "    test_accuracy = nb_classifier.evaluate(test_data)\n",
        "    print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "    #Printing the sizes of the datasets that are used for Training, Validation and Testing.\n",
        "    print(f'Train data size: {len(train_data)}')\n",
        "    print(f'Validation data size: {len(val_data)}')\n",
        "    print(f'Test data size: {len(test_data)}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow5LFIy8I-hs",
        "outputId": "12b60c99-b698-4836-f24f-1c6110220794"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 31.78%\n",
            "Test Accuracy: 31.10%\n",
            "Train data size: 3820\n",
            "Validation data size: 818\n",
            "Test data size: 820\n"
          ]
        }
      ]
    }
  ]
}